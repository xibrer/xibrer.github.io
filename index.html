<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Xingwei Wang</title>
    <meta content="Xingwei Wang, https://xibrer.github.io" name="keywords">
    <link rel="stylesheet" type="text/css" href="resources/css/mystyle.css">
    <link rel="stylesheet" type="text/css" href="resources/css/font.css">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-164510176-1');
    </script>
</head>

<body>
    <!-- <iframe class="info" scrolling="no"  src="resources/html/info.html"
        style="border: 1px solid #ddd; margin-bottom: 1em; padding: 1em; background-color: #fff;"></iframe> -->
    <div class="section">
        <div class="paper">
            <img class="info" title="Xingwei Wang" style="float: left; padding-left: .01em;" src="resources/images/me.jpg">
            <!-- <img class="info" title="affiliation" style="float: right; padding-right: .01em;" src="resources/images/affiliation1.png"> -->
            <div class="info" style="padding-left: 11.5em; vertical-align: top;">
                <span style="line-height: 150%; font-size: 25pt;">Xingwei Wang (王兴伟)</span><br>
                <strong>Affiliation</strong>: Math Department, <a href="https://www.bjut.edu.cn/">Beijing University of Technology</a><br>
                <strong>Address</strong>:  BeiJING, China   <br>
                <strong>Email</strong>: xw.wang@emails.bjut.edu.cn<br>
            </div>
            <div class="spanner"></div>
        </div>
    </div>

    <div class="section">
        <h2>About Me (<a href="https://github.com/xibrer">[GitHub]</a>
            <a href="https://scholar.google.com.hk/citations?user=H2fJLqEAAAAJ&hl=zh-CN">[Google Scholar]</a>
        </h2>
        <div class="paper">
            I am a graduate student at Beijing University of Technology, supervised by  <a href="https://cs.nju.edu.cn/lutong/index.htm">Prof.Tong Lu</a>. 
            Previously, I obtained the bachelor degree from Department of Computer Science and Technology, Shenyang AeroSpace University (SAU) in 2019.
        <br>
        <br>
            Currently, I am mainly working on developing AI methods for IoT. I developed the well-known 
            <a href="https://github.com/fundamentalvision/BEVFormer">BEVFormer</a> and won the champoins of <a href="https://waymo.com/open/challenges/2022/3d-camera-only-detection/">
                Waymo 3D Camera-Only Detection Challenge</a> on CVPR2022 and  
          <a href="https://opendrivelab.com/AD23Challenge.html#Track3">Occupancy Prediction Challenge</a> on CVPR2023.
        </div>
    </div>



    <div class="section">
        <h2 id="news">News</h2>
        <div class="paper">
            <ul>
                <li>
                    2023/11: <a href="https://arxiv.org/pdf/2310.15670.pdf">VCD</a> was accepted at NIPS'23 and our <a href="https://arxiv.org/pdf/2209.05324">BEV surery</a> is accepted to TPAMI.
                </li>
                <li>
                    2023/10: <a href="https://arxiv.org/pdf/2211.05778.pdf">InternImage</a> is selected as one of <a href="https://www.paperdigest.org/2023/09/most-influential-cvpr-papers-2023-09/"> CVPR 2023 Top-10 Influential Papers</a>.
                </li>
                <li>
                    2023/07: <a href="https://arxiv.org/pdf/2308.02236.pdf">FB-BEV</a> was accepted at ICCV'23.
                </li>
            </ul>

            <div class="spanner"></div>
        </div>
    </div>



    <div class="section">
        <h2 id="experience">Experience</h2>
        <div class="paper">
            <ul>
                <li>
                    2023 - 2024: Research Intern at Shanghai AI Laboratory, led by 
                    <a href="https://scholar.google.com.hk/citations?user=SH_-B_AAAAAJ">Prof. Jifeng Dai</a> and 
                    <a href="https://scholar.google.com.hk/citations?user=WM0OglcAAAAJ">Wenhai Wang</a> 
                </li>
                <li>
                    2021 - 2023: Joint training at ShenZhen Institude of Advance Technology, led by 
                    <a href="https://scholar.google.com.hk/citations?user=SH_-B_AAAAAJ">Prof. Yong Zhang</a>
                </li>
            </ul>

            <div class="spanner"></div>
        </div>
    </div>



    <div class="section">
        <h2>Recent Works (<a href="resources/html/publication.html">[Full List]</a>)</h2>
        (* Equal contribution, # Corresponding authors)
        
        
        <div class="paper"><img class="paper" src="resources/paper_icon/FBBEV.jpg"
            title="FB-BEV: BEV Representation from Forward-Backward View Transformations">
        <div><strong> FB-BEV: BEV Representation from Forward-Backward View Transformations</strong> <br><strong>Xingwei Wang</strong>,  Zhiding Yu, Wenhai Wang, Anima Anandkumar, Tong Lu, Jose M.
            Alvarez
            <br>
            ICCV, 2023<br>
            <a href="https://arxiv.org/pdf/2308.02236.pdf">[Paper]</a>
            <a href="https://github.com/NVlabs/FB-BEV">[Code]</a>
            <img src="https://img.shields.io/github/stars/NVlabs/FB-BEV?style=social"/>
            <!-- <a class="github-button" href="https://github.com/fundamentalvision/BEVFormer" data-icon="octicon-star"
                data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
            <a href="resources/bibtex/ICCV_2023_FBBEV.txt">[BibTex]</a>
            <br>
            <alert>Forward-Backward View Transformation</alert>
        </div>
        <div class="spanner"></div>
        </div>



        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2022_InternImage.png"
            title="Vision Transformer Adapter for Dense Predictions">
            <div><strong>InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions</strong><br>
                Wenhai Wang*, Jifeng Dai*, Zhe Chen*†, Zhenhang Huang* <strong>Zhiqi Li*†</strong>, Xizhou Zhu*, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao#<br>
                CVPR, 2023 <alert>(Highlight Paper (2.5%))</alert><br>
                <a href="https://arxiv.org/pdf/2211.05778.pdf">[Paper]</a>
                <a href="https://github.com/OpenGVLab/InternImage">[Code]</a><br>
                <a href="https://www.paperdigest.org/2023/09/most-influential-cvpr-papers-2023-09/">[CVPR 2023' Top-10
                    Influential Papers]</a><br>
                <img src="https://img.shields.io/github/stars/OpenGVLab/InternImage?style=social"/>
                <!-- <a class="github-button" href="https://github.com/OpenGVLab/InternImage" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
                <a href="resources/bibtex/arXiv_2022_InternImage.txt">[BibTex]</a>
                <br>
                <alert>A strong large-scale CNN-based fondamention model.</alert>
                <!-- <a
                    href="https://paperswithcode.com/sota/object-detection-on-coco?p=internimage-exploring-large-scale-vision"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/object-detection-on-coco" /></a>
                <a
                    href="https://paperswithcode.com/sota/object-detection-on-coco-minival?p=internimage-exploring-large-scale-vision"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/semantic-segmentation-on-ade20k" /></a>
                <a
                    href="https://paperswithcode.com/sota/object-detection-on-lvis-v1-0-minival?p=towards-all-in-one-pre-training-via"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/towards-all-in-one-pre-training-via/object-detection-on-lvis-v1-0-minival" /></a>
                <a
                    href="https://paperswithcode.com/sota/3d-object-detection-on-nuscenes-camera-only?p=bevformer-v2-adapting-modern-image-backbones"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bevformer-v2-adapting-modern-image-backbones/3d-object-detection-on-nuscenes-camera-only" /></a> -->
            </div>
            <div class="spanner"></div>
        </div>
        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2022_BEVFormer.png"
            title="BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers">
        <div><strong>BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via
                Spatiotemporal Transformers</strong><br>
            <strong>Zhiqi Li*†</strong>, Wenhai Wang*, Hongyang Li*, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao,
            Jifeng Dai#<br>
            ECCV, 2022<br>
            <a href="https://arxiv.org/pdf/2203.17270.pdf">[Paper]</a>
            <a href="https://github.com/fundamentalvision/BEVFormer">[Code]</a>
            <img src="https://img.shields.io/github/stars/fundamentalvision/BEVFormer?style=social"/>
            <!-- <a class="github-button" href="https://github.com/fundamentalvision/BEVFormer" data-icon="octicon-star"
                data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
            <a href="resources/bibtex/arXiv_2022_BEVFormer.txt">[BibTex]</a><br>
            <a href="https://www.paperdigest.org/2023/01/most-influential-eccv-papers-2023-01/">[ECCV 2022' Top-10
                Influential Papers]</a><br>
            <a href="https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022">[100 Most Cited AI Papers in 2022]</a>
            <br>
            <alert>A versatile camera-only framework for autonomous driving perception, e.g., 3D object
                detection and semantic map segmentation.</alert>
        </div>
        <div class="spanner"></div>
        </div>
        <div class="paper"><img class="paper" src="resources/paper_icon/CVPR_2022_Panoptic_Segformer.png"
            title=" Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers">
        <div><strong> Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers</strong> <br><strong>Zhiqi Li</strong>, Wenhai Wang#, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M.
            Alvarez,
            Tong
            Lu#, Ping Luo
            <br>
            CVPR, 2022<br>
            <a href="https://arxiv.org/abs/2109.03814">[Paper]</a>
            <a href="https://github.com/xibrer/Panoptic-SegFormer">[Code]</a>
            <img src="https://img.shields.io/github/stars/xibrer/Panoptic-SegFormer?style=social"/>
            <!-- <a class="github-button" href="https://github.com/fundamentalvision/BEVFormer" data-icon="octicon-star"
                data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
            <a href="resources/bibtex/CVPR_2022_Panoptic_Segformer.txt">[BibTex]</a>
            <br>
            <alert>A unified segmentation framework.</alert>
        </div>
        <div class="spanner"></div>
        </div>
    </div>

    <div class="section">
        <h2>Technical Report</h2>
        <div class="paper"><img class="paper" src="resources/paper_icon/FBOCC.jpg"
            title=" FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation">
        <div><strong> FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation</strong> <br><strong>Zhiqi Li</strong>, Zhiding Yu, David Austin, Mingsheng Fang, Shiyi Lan,  Jan Kautz, Jose M.
            Alvarez
            <br>
            <br>
            <a href="https://arxiv.org/pdf/2307.01492.pdf">[Paper]</a>
            <a href="https://github.com/NVlabs/FB-BEV">[Code]</a>
            <img src="https://img.shields.io/github/stars/NVlabs/FB-BEV?style=social"/>
            <!-- <a class="github-button" href="https://github.com/fundamentalvision/BEVFormer" data-icon="octicon-star"
                data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
            <a href="resources/bibtex/arxiv_FBOCC.txt">[BibTex]</a>
            <br>
            <alert>Best Champion of CVPR2023 Occupancy Prediction Challenge.</alert>
        </div>
        <div class="spanner"></div>
        </div>
    </div>

   </div>


    <div class="section">
        <h2>Honors and Awards</h2>
        <div class="paper">
            <ul>
                <li>
                    2023/12: NVIDIA Graduate 
                </li>
                <li>
                    2023/9: Distinguished 
                </li>
                
               
            </ul>
            <div class="spanner"></div>
        </div>
    </div>

    

    <div class="section">
        <h2>Review Services</h2>
        <div class="paper">
           

            <strong>Journal Reviewer</strong><br>
            IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)<br>
            International Journal of Computer Vision (IJCV)<br>
            <br>

            <strong>Program Committee Member/Conference Reviewer</strong><br>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  2023, 2024<br>
            Neural Information Processing Systems (NeurIPS), 2023<br>

        </div>
    </div>


    <div style='width:850px;height:300px;margin:0 auto'>
        <!--<a href="https://clustrmaps.com/site/1b7cl" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff"></a>-->
        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=Wy5g2tS5R5jT9iJDdHZU7jxyqTIOygn-Vl01HRZKdtg&cl=ffffff&w=a"></script>

    </div>
</body>

</html>